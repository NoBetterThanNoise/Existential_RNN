{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN module for text generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbSiUeuPxAS7"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import os \n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSJnOzN72ABE"
      },
      "source": [
        "with open('/content/beyond_good_and_evil.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "text = text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZYgWCz64gsT",
        "outputId": "b72cc4ed-a2ea-4e8f-dfda-ad0cc5a41d29"
      },
      "source": [
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 396052 characters\n",
            "\n",
            "\n",
            "\n",
            "chapter i. prejudices of philosophers\n",
            "\n",
            "\n",
            "1. the will to truth, which is to tempt us to many a hazardous\n",
            "enterprise, the famous truthfulness of which all philosophers have\n",
            "hitherto spoken with respect, what questions has this will to truth not\n",
            "laid \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxkKlt5u9bDj",
        "outputId": "a3f159f7-dbe9-4aa4-b6c8-56586907f64b"
      },
      "source": [
        "# Find the unique characters in text to build vocab\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsbOef4G-Ck1"
      },
      "source": [
        "# Create a mapping between unique characters and indices. Essentially a 'look up' table to move between characters and indices\n",
        "\n",
        "char_to_idx = {u:i for i,u in enumerate(vocab)}\n",
        "idx_to_char = np.array(vocab)\n",
        "\n",
        "# Represent each character in text with an integer equivalent\n",
        "text_as_int = np.array([char_to_idx[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KGwgWOE-0cp",
        "outputId": "6f2a40c6-4e39-42ed-d180-9148be430b19"
      },
      "source": [
        "# Characters with their corresponding index value\n",
        "print(\"{\")\n",
        "for char, _ in zip(char_to_idx, range(20)):\n",
        "  print(\" {:4s}: {:3d}, \".format(repr(char), char_to_idx[char]))\n",
        "print(\"  ...\\n}\")\n",
        "\n",
        "print('{} --- characters mapped to int ---- {}'.format(repr(text[:10]), text_as_int[:10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            " '\\n':   0, \n",
            " ' ' :   1, \n",
            " '!' :   2, \n",
            " '\"' :   3, \n",
            " '$' :   4, \n",
            " '%' :   5, \n",
            " \"'\" :   6, \n",
            " '(' :   7, \n",
            " ')' :   8, \n",
            " '*' :   9, \n",
            " ',' :  10, \n",
            " '-' :  11, \n",
            " '.' :  12, \n",
            " '/' :  13, \n",
            " '0' :  14, \n",
            " '1' :  15, \n",
            " '2' :  16, \n",
            " '3' :  17, \n",
            " '4' :  18, \n",
            " '5' :  19, \n",
            "  ...\n",
            "}\n",
            "'\\n\\n\\nchapter' --- characters mapped to int ---- [ 0  0  0 33 38 31 46 50 35 48]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iqsB0_JAlU6",
        "outputId": "14680629-a032-42a4-efc1-34562db89278"
      },
      "source": [
        "# Divde text in example sequence, each input will contain seq_length characters \n",
        "# For each input seq, the corresponding target contains same length, but shifted one char to right\n",
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100 \n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets \n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(20):\n",
        "  print(idx_to_char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "c\n",
            "h\n",
            "a\n",
            "p\n",
            "t\n",
            "e\n",
            "r\n",
            " \n",
            "i\n",
            ".\n",
            " \n",
            "p\n",
            "r\n",
            "e\n",
            "j\n",
            "u\n",
            "d\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhpngHerBp1W",
        "outputId": "f34aa3d6-a0ec-42df-f13a-1bb28a53e7c7"
      },
      "source": [
        "# Batch method lets us convert these induvidual characters to sequences of desired size\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx_to_char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\n\\n\\nchapter i. prejudices of philosophers\\n\\n\\n1. the will to truth, which is to tempt us to many a hazar'\n",
            "'dous\\nenterprise, the famous truthfulness of which all philosophers have\\nhitherto spoken with respect,'\n",
            "' what questions has this will to truth not\\nlaid before us! what strange, perplexing, questionable que'\n",
            "'stions! it is\\nalready a long story; yet it seems as if it were hardly commenced. is\\nit any wonder if '\n",
            "'we at last grow distrustful, lose patience, and turn\\nimpatiently away? that this sphinx teaches us at'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEf4EvvBChUw",
        "outputId": "3dc9799a-b277-4914-9b38-bf8236543f1c"
      },
      "source": [
        "# For each sequence, duplicate and shift it to form the input and target text by using the map method\n",
        "\n",
        "def split_input_target (chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# print first example of input and target values\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print('Input data:  ', repr(''.join(idx_to_char[input_example.numpy()])))\n",
        "  print('Target data: ', repr(''.join(idx_to_char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:   '\\n\\n\\nchapter i. prejudices of philosophers\\n\\n\\n1. the will to truth, which is to tempt us to many a haza'\n",
            "Target data:  '\\n\\nchapter i. prejudices of philosophers\\n\\n\\n1. the will to truth, which is to tempt us to many a hazar'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCw_1_6GDaLk",
        "outputId": "6c57a8d2-389b-4762-b280-ab48654b96fc"
      },
      "source": [
        "# Create training batches by splitting data in managable sequences\n",
        "\n",
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)\n",
        "\n",
        "dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCexaWk8EVHl"
      },
      "source": [
        "# Build Model \n",
        "\n",
        "# Length of vocab in characters\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# the embedding dimensions\n",
        "embedding_dim = 256\n",
        "\n",
        "#number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model (vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                                batch_input_shape = [batch_size, None]),\n",
        "      tf.keras.layers.LSTM(rnn_units, \n",
        "                          return_sequences = True, \n",
        "                          stateful = True, \n",
        "                          recurrent_initializer = 'glorot_uniform'),\n",
        "      tf.keras.layers.Dropout(0.2), \n",
        "      tf.keras.layers.LSTM(rnn_units, \n",
        "                          return_sequences = True, \n",
        "                          stateful = True, \n",
        "                          recurrent_initializer = 'glorot_uniform'), \n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(vocab_size)                        \n",
        "  ])\n",
        "  return model \n",
        "\n",
        "model = build_model(\n",
        "    vocab_size = len(vocab), \n",
        "    embedding_dim = embedding_dim, \n",
        "    rnn_units = rnn_units, \n",
        "    batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MAwqTAMFinD",
        "outputId": "0740fdfc-916a-4e43-eb95-ef03c553a24b"
      },
      "source": [
        "# Test to see if model will run as expected \n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 57) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT4LWbTAGBiP",
        "outputId": "ebfdd7c7-608a-4b33-f695-9d65d076bd7f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           14592     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (64, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (64, None, 1024)          8392704   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (64, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 57)            58425     \n",
            "=================================================================\n",
            "Total params: 13,712,697\n",
            "Trainable params: 13,712,697\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egowYuA1GsYz",
        "outputId": "0c793bed-fb61-4898-a42e-e9e93a089d7c"
      },
      "source": [
        "# Train model using adam optimizer and sparse_catergorical_crossentropy\n",
        "\n",
        "# Because our model returns logits we need to set the from_logits flat\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 57)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.0428247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SkNxOUEHUA4"
      },
      "source": [
        "# Configure checkpoints to ensure checkpoints are saved during training \n",
        "\n",
        "# Directory where checkpoints will be saved \n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpont_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix, \n",
        "    save_weights_only = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y2edYq9IK_l",
        "outputId": "d4ca9494-78af-4faa-8579-35cedd676a1e"
      },
      "source": [
        "# Execute training \n",
        "\n",
        "epochs = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=epochs, callbacks=[checkpont_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "61/61 [==============================] - 1096s 18s/step - loss: 3.0835\n",
            "Epoch 2/20\n",
            "61/61 [==============================] - 1108s 18s/step - loss: 2.5913\n",
            "Epoch 3/20\n",
            "61/61 [==============================] - 1088s 18s/step - loss: 2.2143\n",
            "Epoch 4/20\n",
            "61/61 [==============================] - 1096s 18s/step - loss: 1.9439\n",
            "Epoch 5/20\n",
            "61/61 [==============================] - 1095s 18s/step - loss: 1.7389\n",
            "Epoch 6/20\n",
            "61/61 [==============================] - 1085s 18s/step - loss: 1.5978\n",
            "Epoch 7/20\n",
            "61/61 [==============================] - 1083s 18s/step - loss: 1.4980\n",
            "Epoch 8/20\n",
            "61/61 [==============================] - 1079s 18s/step - loss: 1.4228\n",
            "Epoch 9/20\n",
            "61/61 [==============================] - 1065s 17s/step - loss: 1.3645\n",
            "Epoch 10/20\n",
            "61/61 [==============================] - 1059s 17s/step - loss: 1.3098\n",
            "Epoch 11/20\n",
            "61/61 [==============================] - 1084s 18s/step - loss: 1.2597\n",
            "Epoch 12/20\n",
            "61/61 [==============================] - 1076s 18s/step - loss: 1.2121\n",
            "Epoch 13/20\n",
            "61/61 [==============================] - 1061s 17s/step - loss: 1.1650\n",
            "Epoch 14/20\n",
            "61/61 [==============================] - 1053s 17s/step - loss: 1.1157\n",
            "Epoch 15/20\n",
            "61/61 [==============================] - 1058s 17s/step - loss: 1.0668\n",
            "Epoch 16/20\n",
            "61/61 [==============================] - 1074s 18s/step - loss: 1.0145\n",
            "Epoch 17/20\n",
            "61/61 [==============================] - 1072s 18s/step - loss: 0.9607\n",
            "Epoch 18/20\n",
            "61/61 [==============================] - 1063s 17s/step - loss: 0.9106\n",
            "Epoch 19/20\n",
            "61/61 [==============================] - 1058s 17s/step - loss: 0.8568\n",
            "Epoch 20/20\n",
            "61/61 [==============================] - 1071s 18s/step - loss: 0.8067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6l7Dio7_f4tE",
        "outputId": "f19b5204-7042-4ef8-fa95-068c99359e1f"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_20'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xh0q1JTf-ZB"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fsv5VtsIZb3"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 100\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char_to_idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx_to_char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "migg8W5cewJf",
        "outputId": "7b28a08a-49eb-408b-ec56-e1e4fa4dd075"
      },
      "source": [
        "print(generate_text(model, start_string=u\"the\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ther\n",
            "wich genaine and neighbour inclinations; but there is a common\n",
            "ourselves!\n",
            "this isuro impeas my str\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCIbfTPRe6Ga"
      },
      "source": [
        "while True: pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtuyoSjN3FkD"
      },
      "source": [
        "def save_model(model, suffix=None):\n",
        "  \"\"\"\n",
        "  Saves a given model in a models directory and appends a suffix (str)\n",
        "  for clarity and reuse.\n",
        "  \"\"\"\n",
        "  # Create model directory with current time\n",
        "  modeldir = os.path.join(\"/content/drive/My Drive/Colab Notebooks/Existential_RNN/\")\n",
        "  model_path = modeldir + \"-\" + suffix + \".h5\" # save format of model\n",
        "  print(f\"Saving model to: {model_path}...\")\n",
        "  model.save(model_path)\n",
        "  return model_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "wahnYgAk3m1Y",
        "outputId": "f3eb9296-3e11-482a-ee2e-598799a7400e"
      },
      "source": [
        "save_model(model, suffix=\"Double_LTSM_20_Epochs\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to: /content/drive/My Drive/Colab Notebooks/Existential_RNN/-Double_LTSM_20_Epochs.h5...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/Existential_RNN/-Double_LTSM_20_Epochs.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhXLaeq36C5g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}